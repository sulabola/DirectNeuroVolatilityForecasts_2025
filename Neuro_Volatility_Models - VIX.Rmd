---
title: "NeuroVol - CiFer 2025"
author: "Sulalitha Bowala"
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list=ls(all=TRUE)) # Remove objects from environment
set.seed(123)
```


```{r, message=FALSE}
### Call packages
require("tseries") # load the tseries library
require("zoo")
require("moments")
require("car")
require("MASS")
require("stats")  
require("VGAM")
require("fGarch")
require("quantmod") # To download data alternatively
require("PerformanceAnalytics") # To draw Timeseries graphs
require("scales")
require("PEIP")
require("dplyr")
require("lubridate")
require("tidyverse")
require("gridExtra")
require("gdata")
require("xtable")
require("vioplot")
require("fpp3")
require("readr")
require("tis")
require("tibble")
require("tsibble")
require("stringr")
require("magrittr")
require("fable")
require("Metrics") # rmse()
require("forecast")
```


```{r}
# Download data
start.date = '2022-01-01' # starting date of stock
end.date = '2024-06-05' # ending date of stock

# Download VIX (^VIX) from Yahoo finance
getSymbols("^VIX", src = "yahoo", from = start.date, to = end.date)
Asset <- na.omit(VIX)
# create date variable
Asset <- zoo::fortify.zoo(Asset)
# Rename date variable
Asset <- Asset %>% rename("Date" = "Index")
# create tissble object
Asset <- as_tsibble(Asset, index = Date) # create tissble object
# Re-index based on trading days (as there are some missing days)
Asset_stock <- Asset |>
  mutate(day = row_number()) |>
  update_tsibble(index = day, regular = TRUE)

Asset.Full <- Asset_stock
colnames(Asset.Full) <- c('Date','Open','High','Low','Close','Volume','Adjusted','Day')

plot(Asset.Full$Adjusted, type = "l")
```


### calculate sign correlation (rho)

```{r}
rho.cal<-function(X){
  rho.hat<-cor(sign(X-mean(X)), X-mean(X))
  return(rho.hat)
}
```


### Function for observed volatility

```{r}
observed.vol <- function(X){
  X.cdf <- ecdf(X)
  return(abs(X - mean(X))/(2*rho.cal(X)*sqrt(X.cdf(mean(X))*(1-X.cdf(mean(X))))))
}
```


### Observed Volatility

```{r}
## Calculate volatility for entire data
r <- Asset.Full$Adjusted %>% log %>% diff

rho <- rho.cal(r)
vol <- sqrt(252)*observed.vol(as.numeric(r))

plot(vol, type = "l")

## ggplot

date_sequence <- Asset.Full$Date[-1]

VolatilityFull <- data.frame(
  Date = as.character(date_sequence),
  Obs.Vol = vol
)

library(ggplot2)

# Ensure date column is in Date format
VolatilityFull$Date <- as.Date(VolatilityFull$Date)

# Create the ggplot
ggplot(data = VolatilityFull, aes(x = Date, y = Obs.Vol)) +
  geom_line(color = "black", lwd = 1) +  # Black line for volatility
  labs(
    #title = "Volatility Over Time",  # Add title
    x = "Date",                      # Label for x-axis
    y = "Volatility"                 # Label for y-axis
  ) +
  theme_minimal() +                  # Minimal theme for cleaner look
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") +  # Monthly intervals
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    legend.position = "none"                            # Remove legend if not needed
  )

```


### Summary Statistics of log returns

```{r}
signrho <- rho.cal(r)
fun <- function (x) signrho*(x-1)*beta(x/2,1/2)-2*sqrt(x-2)
nu <- uniroot(fun, c(2, 100))$root

summaryofreturns <- data.frame(Mean = mean(r), SD = sd(r), sign.rho = signrho, K = kurtosis(r), DegreesOfFreedom = nu)
summaryofreturns
```


### Training and Testing data

```{r}
## New data frame for volatility
Asset.Vol <- tsibble(
  Date = Asset_stock$Date[2:(length(vol)+1)],
  Trading_day = 1:length(vol),
  Volatility = vol,
  index = Trading_day
)
head(Asset.Vol)
tail(Asset.Vol)
nrow(Asset.Vol)
```


```{r}
Train <- Asset.Vol[(1:593),]
Test <- Asset.Vol[(594:length(vol)),]


Train.Length = nrow(Train)
Train.Length
Test.Length = nrow(Test)
Test.Length
```


### netar Model - Conditional variance

```{r}
mean.r <- mean(r)

Cond.Variance <- (r-mean.r)^2

### New Data frame for Conditional Variance
Cond.Variance <- tsibble(
  Date = Asset_stock$Date[2:(length(vol)+1)],
  Trading_day = 1:length(vol),
  Cond.Var = Cond.Variance,
  index = Trading_day
)

Train.Cond.Var <- Cond.Variance[(1:593),]
head(Train.Cond.Var)
tail(Train.Cond.Var)

ptm <- proc.time()

nnetar.Model.Cond.Var <- Train.Cond.Var %>%
  model(NNETAR(Cond.Var))

nnetar.Model.Cond.Var

## Forecasts of conditional variance
fore.nnetar.Cond.Var <- nnetar.Model.Cond.Var %>% forecast(h=Test.Length)

fore.nnetar.Cond.Vol <- sqrt(fore.nnetar.Cond.Var$.mean)

proc.time()-ptm

fore.nnetar.Cond.Vol
```





### nnetar Model - Volatility

```{r}

ptm <- proc.time()

nnetar.Model <- Train %>%
  model(NNETAR(Volatility))

nnetar.Model

## Forecasts
fore.nnetar.Vol <- nnetar.Model %>% forecast(h=Test.Length)

proc.time()-ptm
```


### Running neural network models - RNN, LSTM, GRU

### Set path (For Windows computers)

```{r}
library(reticulate)
use_python('C:/Users/sulal/AppData/Local/Programs/Python/Python38/Python.exe')

library(keras)
```


### Run NN models with single layers and with same paramters as in nnetar model

```{r}
## nnetar model parameters

seriesLags <- 8
No.neurons <- 4
```


### RNN Model

```{r}
series <- Train$Volatility

# Normalize the data (this step is optional but recommended)
min_val <- min(series)
max_val <- max(series)
scaled_series <- (series - min_val) / (max_val - min_val)

# Set up parameters
lookback <- seriesLags  # Number of previous time steps used to predict the next one
forecast_length <- Test.Length  # Number of forecasts to generate
train_size <- length(scaled_series)

# Function to create input-output pairs
create_dataset <- function(data, lookback) {
  X <- list()
  y <- list()
  
  for (i in seq(lookback, length(data) - 1)) {
    X[[i - lookback + 1]] <- data[(i - lookback + 1):i]
    y[[i - lookback + 1]] <- data[i + 1]
  }
  
  return(list(X = array(unlist(X), dim = c(length(X), lookback, 1)),
              y = unlist(y)))
}

# Create training data
dataset <- create_dataset(scaled_series, lookback)
X_train <- dataset$X
y_train <- dataset$y

ptm <- proc.time()

# Define the RNN model
RNN.Model <- keras_model_sequential() %>%
  layer_simple_rnn(units = No.neurons, input_shape = c(lookback, 1)) %>%
  layer_dense(units = 1)

# Compile the model
RNN.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the model
RNN.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps
predictions <- numeric(forecast_length)
input_seq <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input <- array(input_seq, dim = c(1, lookback, 1))
  next_val <- RNN.Model %>% predict(pred_input)
  predictions[i] <- next_val
  
  # Update the input sequence
  input_seq <- c(input_seq[-1], next_val)
}

# Reverse scaling to original values
fore.RNN.Vol <- predictions * (max_val - min_val) + min_val
fore.RNN.Vol

proc.time()-ptm

```


### LSTM Model

```{r}

ptm <- proc.time()

# Define the LSTM model (changing only the architecture)
LSTM.Model <- keras_model_sequential() %>%
  layer_lstm(units = No.neurons, input_shape = c(lookback, 1)) %>%
  layer_dense(units = 1)

# Compile the LSTM model
LSTM.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the LSTM model
LSTM.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the LSTM model
lstm_predictions <- numeric(forecast_length)
input_seq_lstm <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_lstm <- array(input_seq_lstm, dim = c(1, lookback, 1))
  next_val_lstm <- LSTM.Model %>% predict(pred_input_lstm)
  lstm_predictions[i] <- next_val_lstm
  
  # Update the input sequence
  input_seq_lstm <- c(input_seq_lstm[-1], next_val_lstm)
}

# Reverse scaling to original values for LSTM predictions
fore.LSTM.Vol <- lstm_predictions * (max_val - min_val) + min_val
fore.LSTM.Vol

proc.time()-ptm
```


### GRU Model

```{r}

ptm <- proc.time()

# Define the GRU model (changing only the architecture)
GRU.Model <- keras_model_sequential() %>%
  layer_gru(units = No.neurons, input_shape = c(lookback, 1)) %>%
  layer_dense(units = 1)

# Compile the GRU model
GRU.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the GRU model
GRU.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the GRU model
gru_predictions <- numeric(forecast_length)
input_seq_gru <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_gru <- array(input_seq_gru, dim = c(1, lookback, 1))
  next_val_gru <- GRU.Model %>% predict(pred_input_gru)
  gru_predictions[i] <- next_val_gru
  
  # Update the input sequence
  input_seq_gru <- c(input_seq_gru[-1], next_val_gru)
}

# Reverse scaling to original values for GRU predictions
fore.GRU.Vol <- gru_predictions * (max_val - min_val) + min_val
fore.GRU.Vol

proc.time()-ptm

```


### Forecasts Errors

```{r}

## nnetar - Conditional volatility
RMSE.nnetar.Cond.Vol <- sqrt(mean((Test$Volatility-fore.nnetar.Cond.Vol)^2))
MAE.nnetar.Cond.Vol <- mean(abs(Test$Volatility-fore.nnetar.Cond.Vol))
MAPE.nnetar.Cond.Vol <- mean(abs(100*(Test$Volatility-fore.nnetar.Cond.Vol)/Test$Volatility))


## nnetar - volatility
RMSE.nnetar.Vol <- sqrt(mean((Test$Volatility-fore.nnetar.Vol$.mean)^2))
MAE.nnetar.Vol <- mean(abs(Test$Volatility-fore.nnetar.Vol$.mean))
MAPE.nnetar.Vol <- mean(abs(100*(Test$Volatility-fore.nnetar.Vol$.mean)/Test$Volatility))

## RNN
RMSE.RNN <- sqrt(mean((Test$Volatility-fore.RNN.Vol)^2))
MAE.RNN <- mean(abs(Test$Volatility-fore.RNN.Vol))
MAPE.RNN <- mean(abs(100*(Test$Volatility-fore.RNN.Vol)/Test$Volatility))

## LSTM
RMSE.LSTM <- sqrt(mean((Test$Volatility-fore.LSTM.Vol)^2))
MAE.LSTM <- mean(abs(Test$Volatility-fore.LSTM.Vol))
MAPE.LSTM <- mean(abs(100*(Test$Volatility-fore.LSTM.Vol)/Test$Volatility))

## GRU
RMSE.GRU <- sqrt(mean((Test$Volatility-fore.GRU.Vol)^2))
MAE.GRU <- mean(abs(Test$Volatility-fore.GRU.Vol))
MAPE.GRU <- mean(abs(100*(Test$Volatility-fore.GRU.Vol)/Test$Volatility))

## Summarize
Models <- c('GARCH.Neuro.Volatility','DD.Neuro.Volatility','RNN','LSTM','GRU')
RMSE <- c(RMSE.nnetar.Cond.Vol,RMSE.nnetar.Vol,RMSE.RNN,RMSE.LSTM,RMSE.GRU)
MAE <- c(MAE.nnetar.Cond.Vol,MAE.nnetar.Vol,MAE.RNN,MAE.LSTM,MAE.GRU)
MAPE <- c(MAPE.nnetar.Cond.Vol,MAPE.nnetar.Vol,MAPE.RNN,MAPE.LSTM,MAPE.GRU)

data.frame(Models,RMSE,MAE,MAPE)
```


### Print point  Volatility Forecasts in one plot

### Merge and create a new dataframe for volatility forecasts

```{r}
# Define start and end dates
start_date <- as.Date(start.date)
end_date <- as.Date(end.date)

# Create a sequence of dates
date_sequence <- seq(start_date, end_date, by = "day")
#date_sequence

# Number of last values to select
nTemp <- Test.Length

# Select the last 'n' values from the vector
TestDates <- date_sequence[(length(date_sequence) - nTemp + 1):length(date_sequence)]

VolForecasts <- data.frame(
  Date = as.character(TestDates),
  Obs.Vol = Test$Volatility,
  nnetar.Cond.Vol <- fore.nnetar.Cond.Vol,
  nnetar.Vol = fore.nnetar.Vol$.mean,
  RNN.Vol = fore.RNN.Vol,
  LSTM.Vol = fore.LSTM.Vol,
  GRU.Vol = fore.GRU.Vol
)
```


### Plot

```{r}
# Create the plot with date interval
ggplot(data = VolForecasts, aes(x = as.Date(Date))) +
  geom_line(aes(y = Obs.Vol, color = "Observed"), lwd = 1.5) +
  geom_line(aes(y = nnetar.Cond.Vol, color = "nnetar.Cond.Vol"), lwd = 1.5) +
  geom_line(aes(y = nnetar.Vol, color = "nnetar.Vol"), lwd = 1.5) +
  geom_line(aes(y = RNN.Vol, color = "RNN"), lwd = 1.5) + 
  geom_line(aes(y = LSTM.Vol, color = "LSTM"), lwd = 1.5) + 
  geom_line(aes(y = GRU.Vol, color = "GRU"), lwd = 1.5) +
  labs(y = "Volatility",
       x = "Date") +
  scale_color_manual(name = "",
                     values = c("Observed" = "black",
                                "nnetar.Cond.Vol" = "purple",
                                "nnetar.Vol" = "red", 
                                "RNN" = "green",
                                "LSTM" = "orange",
                                "GRU" = "blue"), # Add color for the new series
                     labels = c("Observed" = "Observed Volatility",
                                "nnetar.Cond.Vol" = "GARCH.Neuro.Volatility",
                                "nnetar.Vol" = "DD.Neuro.Volatility", 
                                "RNN" = "RNN",
                                "LSTM" = "LSTM",
                                "GRU" = "GRU")) + # Adjust labels
  scale_x_date(date_labels = "%Y-%m-%d") + # Show dates at monthly intervals
  theme_minimal() +
  theme(legend.position = c(0.3, 0.80))  # Place the legend at coordinates (0.9, 0.9) within the plotting area
```


### Run NN models with multiple layers (2 layers, each layer same number of nodes as in nnetar)


### RNN model with 2 layers


```{r}

ptm <- proc.time()

# Define the new RNN model with two hidden layers
RNN.Model.TwoLayer <- keras_model_sequential() %>%
  layer_simple_rnn(units = No.neurons, input_shape = c(lookback, 1), return_sequences = TRUE) %>%
  layer_simple_rnn(units = No.neurons) %>%
  layer_dense(units = 1)

# Compile the model
RNN.Model.TwoLayer %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the new model
RNN.Model.TwoLayer %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps
predictions_two_layer <- numeric(forecast_length)
input_seq <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input <- array(input_seq, dim = c(1, lookback, 1))
  next_val <- RNN.Model.TwoLayer %>% predict(pred_input)
  predictions_two_layer[i] <- next_val
  
  # Update the input sequence
  input_seq <- c(input_seq[-1], next_val)
}

# Reverse scaling to original values
fore.RNN.TwoLayer.Vol <- predictions_two_layer * (max_val - min_val) + min_val
fore.RNN.TwoLayer.Vol

proc.time()-ptm

```


### LSTM with 2 layers

```{r}

ptm <- proc.time()

# Define the two-layer LSTM model
LSTM.Model.TwoLayer <- keras_model_sequential() %>%
  layer_lstm(units = No.neurons, input_shape = c(lookback, 1), return_sequences = TRUE) %>%
  layer_lstm(units = No.neurons) %>%
  layer_dense(units = 1)

# Compile the LSTM model
LSTM.Model.TwoLayer %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the LSTM model
LSTM.Model.TwoLayer %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the two-layer LSTM model
lstm_predictions_two_layer <- numeric(forecast_length)
input_seq_lstm <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_lstm <- array(input_seq_lstm, dim = c(1, lookback, 1))
  next_val_lstm <- LSTM.Model.TwoLayer %>% predict(pred_input_lstm)
  lstm_predictions_two_layer[i] <- next_val_lstm
  
  # Update the input sequence
  input_seq_lstm <- c(input_seq_lstm[-1], next_val_lstm)
}

# Reverse scaling to original values for LSTM predictions
fore.LSTM.TwoLayer.Vol <- lstm_predictions_two_layer * (max_val - min_val) + min_val
fore.LSTM.TwoLayer.Vol

proc.time()-ptm

```



### GRU with 2 layers

```{r}

ptm <- proc.time()

# Define the two-layer GRU model
GRU.Model.TwoLayer <- keras_model_sequential() %>%
  layer_gru(units = No.neurons, input_shape = c(lookback, 1), return_sequences = TRUE) %>%
  layer_gru(units = No.neurons) %>%
  layer_dense(units = 1)

# Compile the GRU model
GRU.Model.TwoLayer %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the GRU model
GRU.Model.TwoLayer %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the two-layer GRU model
gru_predictions_two_layer <- numeric(forecast_length)
input_seq_gru <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_gru <- array(input_seq_gru, dim = c(1, lookback, 1))
  next_val_gru <- GRU.Model.TwoLayer %>% predict(pred_input_gru)
  gru_predictions_two_layer[i] <- next_val_gru
  
  # Update the input sequence
  input_seq_gru <- c(input_seq_gru[-1], next_val_gru)
}

# Reverse scaling to original values for GRU predictions
fore.GRU.TwoLayer.Vol <- gru_predictions_two_layer * (max_val - min_val) + min_val
fore.GRU.TwoLayer.Vol

proc.time()-ptm

```


### Forecasts Errors

```{r}

## RNN with 2 layers
RMSE.RNN.TwoLayer <- sqrt(mean((Test$Volatility-fore.RNN.TwoLayer.Vol)^2))
MAE.RNN.TwoLayer <- mean(abs(Test$Volatility-fore.RNN.TwoLayer.Vol))
MAPE.RNN.TwoLayer <- mean(abs(100*(Test$Volatility-fore.RNN.TwoLayer.Vol)/Test$Volatility))

## LSTM with 2 layers
RMSE.LSTM.TwoLayer <- sqrt(mean((Test$Volatility-fore.LSTM.TwoLayer.Vol)^2))
MAE.LSTM.TwoLayer <- mean(abs(Test$Volatility-fore.LSTM.TwoLayer.Vol))
MAPE.LSTM.TwoLayer <- mean(abs(100*(Test$Volatility-fore.LSTM.TwoLayer.Vol)/Test$Volatility))

## GRU with 2 layers
RMSE.GRU.TwoLayer <- sqrt(mean((Test$Volatility-fore.GRU.TwoLayer.Vol)^2))
MAE.GRU.TwoLayer <- mean(abs(Test$Volatility-fore.GRU.TwoLayer.Vol))
MAPE.GRU.TwoLayer <- mean(abs(100*(Test$Volatility-fore.GRU.TwoLayer.Vol)/Test$Volatility))

## Summarize with 2 layers
Models.TwoLayer <- c('GARCH.Neuro.Volatility','DD.Neuro.Volatility','RNN.TwoLayer','LSTM.TwoLayer','GRU.TwoLayer')
RMSE.TwoLayer <- c(RMSE.nnetar.Cond.Vol,RMSE.nnetar.Vol,RMSE.RNN.TwoLayer,RMSE.LSTM.TwoLayer,RMSE.GRU.TwoLayer)
MAE.TwoLayer <- c(MAE.nnetar.Cond.Vol,MAE.nnetar.Vol,MAE.RNN.TwoLayer,MAE.LSTM.TwoLayer,MAE.GRU.TwoLayer)
MAPE.TwoLayer <- c(MAPE.nnetar.Cond.Vol,MAPE.nnetar.Vol,MAPE.RNN.TwoLayer,MAPE.LSTM.TwoLayer,MAPE.GRU.TwoLayer)

data.frame(Models.TwoLayer,RMSE.TwoLayer,MAE.TwoLayer,MAPE.TwoLayer)
```


### Print point Volatility Forecasts in one plot - 2 layers

### Merge and create a new dataframe for volatility forecasts

```{r}

VolForecasts.TwoLayer <- data.frame(
  Date = as.character(TestDates),
  Obs.Vol = Test$Volatility,
  nnetar.Vol = fore.nnetar.Vol$.mean,
  RNN.Vol.TwoLayer = fore.RNN.TwoLayer.Vol,
  LSTM.Vol.TwoLayer = fore.LSTM.TwoLayer.Vol,
  GRU.Vol.TwoLayer = fore.GRU.TwoLayer.Vol
)
```


### Plot

```{r}

# Create the plot with date interval
ggplot(data = VolForecasts.TwoLayer, aes(x = as.Date(Date))) +
  geom_line(aes(y = Obs.Vol, color = "Observed"), lwd = 1.5) +
  geom_line(aes(y = nnetar.Cond.Vol, color = "nnetar.Cond.Vol"), lwd = 1.5) +
  geom_line(aes(y = nnetar.Vol, color = "nnetar.Vol"), lwd = 1.5) +
  geom_line(aes(y = RNN.Vol.TwoLayer, color = "RNN.TwoLayer"), lwd = 1.5) + 
  geom_line(aes(y = LSTM.Vol.TwoLayer, color = "LSTM.TwoLayer"), lwd = 1.5) + 
  geom_line(aes(y = GRU.Vol.TwoLayer, color = "GRU.TwoLayer"), lwd = 1.5) +
  labs(y = "Volatility",
       x = "Date") +
  scale_color_manual(name = "",
                     values = c("Observed" = "black",
                                "nnetar.Cond.Vol" = "purple",
                                "nnetar.Vol" = "red",
                                "RNN.TwoLayer" = "green",
                                "LSTM.TwoLayer" = "orange",
                                "GRU.TwoLayer" = "blue"), # Add color for the new series
                     labels = c("Observed" = "Observed Volatility",
                                "nnetar.Cond.Vol" = "GARCH.Neuro.Volatility",
                                "nnetar.Vol" = "DD.Neuro.Volatility",
                                "RNN.TwoLayer" = "RNN",
                                "LSTM.TwoLayer" = "LSTM",
                                "GRU.TwoLayer" = "GRU")) + # Adjust labels
  scale_x_date(date_labels = "%Y-%m-%d") + # Show dates at monthly intervals
  theme_minimal() +
  theme(legend.position = c(0.3, 0.80))  # Place the legend at coordinates (0.9, 0.9) within the plotting area
```


### Bidirectional LSTM model for data - 1 hidden layer

```{r}

ptm <- proc.time()

# Define the Bidirectional LSTM model
BiLSTM.Model <- keras_model_sequential() %>%
  bidirectional(layer_lstm(units = No.neurons, input_shape = c(lookback, 1))) %>%
  layer_dense(units = 1)

# Compile the Bidirectional LSTM model
BiLSTM.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the Bidirectional LSTM model
BiLSTM.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the Bidirectional LSTM model
bi_lstm_predictions <- numeric(forecast_length)
input_seq_bi_lstm <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_bi_lstm <- array(input_seq_bi_lstm, dim = c(1, lookback, 1))
  next_val_bi_lstm <- BiLSTM.Model %>% predict(pred_input_bi_lstm)
  bi_lstm_predictions[i] <- next_val_bi_lstm
  
  # Update the input sequence
  input_seq_bi_lstm <- c(input_seq_bi_lstm[-1], next_val_bi_lstm)
}

# Reverse scaling to original values for Bidirectional LSTM predictions
fore.BiLSTM.Vol <- bi_lstm_predictions * (max_val - min_val) + min_val
fore.BiLSTM.Vol

proc.time()-ptm

```


### Bidirectional GRU Model - 1 hidden layer


```{r}

ptm <- proc.time()

# Define the Bidirectional GRU model
BiGRU.Model <- keras_model_sequential() %>%
  bidirectional(layer_gru(units = No.neurons, input_shape = c(lookback, 1))) %>%
  layer_dense(units = 1)

# Compile the Bidirectional GRU model
BiGRU.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the Bidirectional GRU model
BiGRU.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the Bidirectional GRU model
bi_gru_predictions <- numeric(forecast_length)
input_seq_bi_gru <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_bi_gru <- array(input_seq_bi_gru, dim = c(1, lookback, 1))
  next_val_bi_gru <- BiGRU.Model %>% predict(pred_input_bi_gru)
  bi_gru_predictions[i] <- next_val_bi_gru
  
  # Update the input sequence
  input_seq_bi_gru <- c(input_seq_bi_gru[-1], next_val_bi_gru)
}

# Reverse scaling to original values for Bidirectional GRU predictions
fore.BiGRU.Vol <- bi_gru_predictions * (max_val - min_val) + min_val
fore.BiGRU.Vol

proc.time()-ptm

```


### Bidirectional LSTM Model - 2 hidden layer


```{r}

ptm <- proc.time()

# Define the Bidirectional LSTM model with two hidden layers
BiLSTM.TwoLayer.Model <- keras_model_sequential() %>%
  bidirectional(layer_lstm(units = No.neurons, return_sequences = TRUE, input_shape = c(lookback, 1))) %>%
  bidirectional(layer_lstm(units = No.neurons)) %>%
  layer_dense(units = 1)

# Compile the Bidirectional LSTM model
BiLSTM.TwoLayer.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the Bidirectional LSTM model
BiLSTM.TwoLayer.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the Bidirectional LSTM model
bi_lstm_two_layer_predictions <- numeric(forecast_length)
input_seq_bi_lstm_two_layer <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_bi_lstm_two_layer <- array(input_seq_bi_lstm_two_layer, dim = c(1, lookback, 1))
  next_val_bi_lstm_two_layer <- BiLSTM.TwoLayer.Model %>% predict(pred_input_bi_lstm_two_layer)
  bi_lstm_two_layer_predictions[i] <- next_val_bi_lstm_two_layer
  
  # Update the input sequence
  input_seq_bi_lstm_two_layer <- c(input_seq_bi_lstm_two_layer[-1], next_val_bi_lstm_two_layer)
}

# Reverse scaling to original values for Bidirectional LSTM predictions
fore.BiLSTM.TwoLayer.Vol <- bi_lstm_two_layer_predictions * (max_val - min_val) + min_val
fore.BiLSTM.TwoLayer.Vol

proc.time()-ptm

```


### Bidirectional GRU Model - 2 hidden layers

```{r}

ptm <- proc.time()

# Define the Bidirectional GRU model with two hidden layers
BiGRU.TwoLayer.Model <- keras_model_sequential() %>%
  bidirectional(layer_gru(units = No.neurons, return_sequences = TRUE, input_shape = c(lookback, 1))) %>%
  bidirectional(layer_gru(units = No.neurons)) %>%
  layer_dense(units = 1)

# Compile the Bidirectional GRU model
BiGRU.TwoLayer.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the Bidirectional GRU model
BiGRU.TwoLayer.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32
)

# Forecast the next Test.Length steps using the Bidirectional GRU model
bi_gru_two_layer_predictions <- numeric(forecast_length)
input_seq_bi_gru_two_layer <- scaled_series[(train_size - lookback + 1):train_size]

for (i in 1:forecast_length) {
  pred_input_bi_gru_two_layer <- array(input_seq_bi_gru_two_layer, dim = c(1, lookback, 1))
  next_val_bi_gru_two_layer <- BiGRU.TwoLayer.Model %>% predict(pred_input_bi_gru_two_layer)
  bi_gru_two_layer_predictions[i] <- next_val_bi_gru_two_layer
  
  # Update the input sequence
  input_seq_bi_gru_two_layer <- c(input_seq_bi_gru_two_layer[-1], next_val_bi_gru_two_layer)
}

# Reverse scaling to original values for Bidirectional GRU predictions
fore.BiGRU.TwoLayer.Vol <- bi_gru_two_layer_predictions * (max_val - min_val) + min_val
fore.BiGRU.TwoLayer.Vol

proc.time()-ptm

```


### Forecasts Errors

```{r}

# Bidirectional LSTM with 1 hidden layer
RMSE.BiLSTM <- sqrt(mean((Test$Volatility - fore.BiLSTM.Vol)^2))
MAE.BiLSTM <- mean(abs(Test$Volatility - fore.BiLSTM.Vol))
MAPE.BiLSTM <- mean(abs(100 * (Test$Volatility - fore.BiLSTM.Vol) / Test$Volatility))

# Bidirectional GRU with 1 hidden layer
RMSE.BiGRU <- sqrt(mean((Test$Volatility - fore.BiGRU.Vol)^2))
MAE.BiGRU <- mean(abs(Test$Volatility - fore.BiGRU.Vol))
MAPE.BiGRU <- mean(abs(100 * (Test$Volatility - fore.BiGRU.Vol) / Test$Volatility))

# Bidirectional LSTM with 2 hidden layers
RMSE.BiLSTM.TwoLayer <- sqrt(mean((Test$Volatility - fore.BiLSTM.TwoLayer.Vol)^2))
MAE.BiLSTM.TwoLayer <- mean(abs(Test$Volatility - fore.BiLSTM.TwoLayer.Vol))
MAPE.BiLSTM.TwoLayer <- mean(abs(100 * (Test$Volatility - fore.BiLSTM.TwoLayer.Vol) / Test$Volatility))

# Bidirectional GRU with 2 hidden layers
RMSE.BiGRU.TwoLayer <- sqrt(mean((Test$Volatility - fore.BiGRU.TwoLayer.Vol)^2))
MAE.BiGRU.TwoLayer <- mean(abs(Test$Volatility - fore.BiGRU.TwoLayer.Vol))
MAPE.BiGRU.TwoLayer <- mean(abs(100 * (Test$Volatility - fore.BiGRU.TwoLayer.Vol) / Test$Volatility))

# Summarize results
Models <- c('GARCH.Neuro.Volatility','DD.Neuro.Volatility', 'BiLSTM', 'BiGRU', 'BiLSTM.TwoLayer', 'BiGRU.TwoLayer')
RMSE <- c(RMSE.nnetar.Cond.Vol,RMSE.nnetar.Vol, RMSE.BiLSTM, RMSE.BiGRU, RMSE.BiLSTM.TwoLayer, RMSE.BiGRU.TwoLayer)
MAE <- c(MAE.nnetar.Cond.Vol,MAE.nnetar.Vol, MAE.BiLSTM, MAE.BiGRU, MAE.BiLSTM.TwoLayer, MAE.BiGRU.TwoLayer)
MAPE <- c(MAPE.nnetar.Cond.Vol,MAPE.nnetar.Vol, MAPE.BiLSTM, MAPE.BiGRU, MAPE.BiLSTM.TwoLayer, MAPE.BiGRU.TwoLayer)

# Create a data frame to summarize the errors
data.frame(Models, RMSE, MAE, MAPE)

```


### Print point Volatility Forecasts in one plot - Bidirectional models

### Merge and create a new dataframe for volatility forecasts

```{r}

VolForecasts.Bidirectional <- data.frame(
  Date = as.character(TestDates),
  Obs.Vol = Test$Volatility,
  nnetar.Vol = fore.nnetar.Vol$.mean,
  BiLSTM.Vol = fore.BiLSTM.Vol,
  BiGRU.Vol = fore.BiGRU.Vol,
  BiLSTM.Vol.TwoLayer = fore.BiLSTM.TwoLayer.Vol,
  BiGRU.Vol.TwoLayer = fore.BiGRU.TwoLayer.Vol
)
```


### Plot

```{r}

# Create the plot with date interval, comparing bidirectional LSTM and GRU models
ggplot(data = VolForecasts.Bidirectional, aes(x = as.Date(Date))) +
  geom_line(aes(y = Obs.Vol, color = "Observed"), lwd = 1.5) +
  geom_line(aes(y = nnetar.Cond.Vol, color = "nnetar.Cond.Vol"), lwd = 1.5) +
  geom_line(aes(y = nnetar.Vol, color = "nnetar.Vol"), lwd = 1.5) +
  geom_line(aes(y = BiLSTM.Vol, color = "BiLSTM"), lwd = 1.5) +  # Bidirectional LSTM (1 layer)
  geom_line(aes(y = BiGRU.Vol, color = "BiGRU"), lwd = 1.5) +    # Bidirectional GRU (1 layer)
  geom_line(aes(y = BiLSTM.Vol.TwoLayer, color = "BiLSTM.TwoLayer"), lwd = 1.5) +  # Bidirectional LSTM (2 layers)
  geom_line(aes(y = BiGRU.Vol.TwoLayer, color = "BiGRU.TwoLayer"), lwd = 1.5) +    # Bidirectional GRU (2 layers) 
  labs(y = "Volatility",
       x = "Date") +
  scale_color_manual(name = "",
                     values = c("Observed" = "black",
                                "nnetar.Cond.Vol" = "purple",
                                "nnetar.Vol" = "red",
                                "BiLSTM" = "yellow",           # Use same color as LSTM
                                "BiGRU" = "blue",             # Use same color as GRU
                                "BiLSTM.TwoLayer" = "darkorange",  # Darker shade for LSTM (2 layers)
                                "BiGRU.TwoLayer" = "darkblue"),    # Darker shade for GRU (2 layers)
                     labels = c("Observed" = "Observed Volatility",
                                "nnetar.Cond.Vol" = "GARCH.Neuro.Volatility",
                                "nnetar.Vol" = "DD.Neuro.Volatility",
                                "BiLSTM" = "Bidirectional LSTM (1 layer)",
                                "BiGRU" = "Bidirectional GRU (1 layer)",
                                "BiLSTM.TwoLayer" = "Bidirectional LSTM (2 layers)",
                                "BiGRU.TwoLayer" = "Bidirectional GRU (2 layers)")) +
  scale_x_date(date_labels = "%Y-%m-%d") +
  theme_minimal() +
  theme(legend.position = c(0.3, 0.80))  # Keep the legend at the same position

```


